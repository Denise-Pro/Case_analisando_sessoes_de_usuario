# -*- coding: utf-8 -*-
"""case_Escale_Ate_atapa2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gaXWOtharhVhN640Y6fCmrMIhiJ7M6dJ
"""

# instalar as dependências
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark

# configurar as variáveis de ambiente
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

# tornar o pyspark "importável"
import findspark
findspark.init('spark-2.4.4-bin-hadoop2.7')


from pyspark.sql import SparkSession
sc = SparkSession.builder.master('local[*]').getOrCreate()
from pyspark.sql import *

# download do http para arquivo local
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00000.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00001.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00002.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00003.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00004.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00005.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00006.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00007.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00008.json.gz
!wget --quiet --show-progress https://d3l36jjwr70u5l.cloudfront.net/data-engineer-test/part-00009.json.gz

from pyspark.sql import functions as f
from pyspark.sql import types as t

# carregando dados dos arquivos json em dataframes
df_spark_json_0 = sc.read.json("./part-00000.json.gz")
df_spark_json_1 = sc.read.json("./part-00001.json.gz")
df_spark_json_2 = sc.read.json("./part-00002.json.gz")
df_spark_json_3 = sc.read.json("./part-00003.json.gz")
df_spark_json_4 = sc.read.json("./part-00004.json.gz")
df_spark_json_5 = sc.read.json("./part-00005.json.gz")
df_spark_json_6 = sc.read.json("./part-00006.json.gz")
df_spark_json_7 = sc.read.json("./part-00007.json.gz")
df_spark_json_8 = sc.read.json("./part-00008.json.gz")
df_spark_json_9 = sc.read.json("./part-00009.json.gz")

# converto aqui o formato epoch da coluna device_sent_timestamp para um formato timestamp legível

df_spark_jdf_spark_json_0 = df_spark_json_0.withColumn('device_sent_timestamp',(df_spark_json_0.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_1 = df_spark_json_1.withColumn('device_sent_timestamp',(df_spark_json_1.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_2 = df_spark_json_2.withColumn('device_sent_timestamp',(df_spark_json_2.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_3 = df_spark_json_3.withColumn('device_sent_timestamp',(df_spark_json_3.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_4 = df_spark_json_4.withColumn('device_sent_timestamp',(df_spark_json_4.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_5 = df_spark_json_5.withColumn('device_sent_timestamp',(df_spark_json_5.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_6 = df_spark_json_6.withColumn('device_sent_timestamp',(df_spark_json_6.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_7 = df_spark_json_7.withColumn('device_sent_timestamp',(df_spark_json_7.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_8 = df_spark_json_8.withColumn('device_sent_timestamp',(df_spark_json_8.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df_spark_jdf_spark_json_9 = df_spark_json_9.withColumn('device_sent_timestamp',(df_spark_json_9.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))

# faço aqui a uniaõ de todos os arquivos particionados
df = df_spark_json_0.union(df_spark_json_1).union(df_spark_json_2)\
  .union(df_spark_json_3).union(df_spark_json_4).union(df_spark_json_5)\
  .union(df_spark_json_6).union(df_spark_json_7).union(df_spark_json_8).union(df_spark_json_9)
df = df.withColumn('device_sent_timestamp',(df.device_sent_timestamp/1000).cast("timestamp").alias("timestamp"))
df.show()

df_spark_jdf_spark_json_9

df_spark_jdf_spark_json_9.show()

"""Através do agrupamento de id de usuários, Identifico aqui se há algum id de usuário que se repita - em todos os dataframes"""

df_spark_jdf_spark_json_9.groupBy('anonymous_id').count().show()

"""#**Etapa 1**
### Ao perceber que não há repetições nas colunas de usuário('anonymous_id') em nenhum dos arquivos particionados e que não há um tempo inicial e final para um mesmo usuário, para que eu possa contabilizar o tempo de uma sessão, eu assumo que cada linha da coluna 'anonymous id' é uma sessão de usuário diferente. Sendo assim , uma contagem nessa coluna me dará o número total de sessões únicas
"""

dict = {}
# count = df_spark_json_0.select(df_spark_json_0.anonymous_id).count()
dict.update({'part-00000.json.gz': df_spark_json_0.select('anonymous_id').count()})
dict.update({'part-00001.json.gz': df_spark_json_1.select('anonymous_id').count()})
dict.update({'part-00002.json.gz': df_spark_json_2.select('anonymous_id').count()})
dict.update({'part-00003.json.gz': df_spark_json_3.select('anonymous_id').count()})
dict.update({'part-00004.json.gz': df_spark_json_4.select('anonymous_id').count()})
dict.update({'part-00005.json.gz': df_spark_json_5.select('anonymous_id').count()})
dict.update({'part-00006.json.gz': df_spark_json_6.select('anonymous_id').count()})
dict.update({'part-00007.json.gz': df_spark_json_7.select('anonymous_id').count()})
dict.update({'part-00008.json.gz': df_spark_json_8.select('anonymous_id').count()})
dict.update({'part-00009.json.gz': df_spark_json_9.select('anonymous_id').count()})
dict

"""#**Etapa 2**"""

# faço aqui a uniaõ de todos os arquivos particionados
df = df_spark_json_0.union(df_spark_json_1).union(df_spark_json_2)\
  .union(df_spark_json_3).union(df_spark_json_4).union(df_spark_json_5)\
  .union(df_spark_json_6).union(df_spark_json_7).union(df_spark_json_8).union(df_spark_json_9)
df.show()

#verifico se a quantidade de linhas desse novo dataframe bate com a quantidade de linhas somadas de todos os arquivos particionados - 
# cada file tem aproximadamente 10 milhoes de lihas - a qtd bate com o esperado!
df.count()

df_br = df.groupBy('browser_family').count().toPandas()

df_br

list_browser = df_br.browser_family.values
list_browser_values = df_br['count'].values

dict_browser = {}
for i in range(len(list_browser)):
  dict_browser.update({list_browser[i]: list_browser_values[i]})
dict_browser

df_device = df.groupBy('device_family').count().toPandas()

df_device

list_devices = df_device.device_family.values
list_devices_values =  df_device['count'].values

dict_devices = {}
for i in range(len(list_devices)):
  dict_devices.update({list_devices[i]:list_devices_values[i]})
dict_devices

df_os = df.groupBy('os_family').count().toPandas()

df_os

import pandas as pd
list_os = df_os.os_family.values
list_os_values =  df_os['count'].values

dict_os = {}
for i in range(len(list_os)):
  dict_os.update({list_os[i]: list_os_values[i]})
dict_os

"""# **Resultado Etapa 2**
Condenso aqui a quantidade de sessões únicas que ocorreram em cada Browser, Sistema Operacional e Dispositivo dentro de todo o conjunto de dados no formato solicitado
"""

dict_etapa2 = {}
dict_etapa2.update({"browser_family": dict_browser,"os_family": dict_os, "device_family": dict_devices})
dict_etapa2

dict_etapa2.keys()
# dict_etapa2.values()